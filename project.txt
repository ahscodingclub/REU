Leverage label uncertainty to make more confident and reliable diagnosis using the LIDC (JF)

1) Label Uncertainty 
  Aggregate Opinions
  mode, mean, median (probabilistic label vector -- plv)
  a distrobution of labels 
2) look into classifiers with plv input/plv ouput 
3) predictions, confidence (co), credibility (cr), reliability (re) 
  
  make a matrix of what fields these metrics appear in, how they are used, the methodology 
    of generating the values, in hope of connecting the dots and brining other fields work
    into ours

4) Interpretation of output
  How many cases are predicted with . . . 
    high co,cr,re
    low co,cr,re 
  are some cases easier than others? what about the TYPICALITY of a case?

5) GUI to actaully integrate this approach into the field

  
AUCdt -- area under curve distance threshold
ROC -- Reciever Operating Characteristic

Regarding your question, the main difference between ROC/AUC and CP is that ROC/AUC only provides you 
  with an average test accuracy about the classifier in general. However, CP provides a specific confidence 
  measure regarding each example you classified 

“credibility” which serves as an indicator of the reliability of the data upon which we make our prediction
  -- Saunders, Craig, Alexander Gammerman, and Volodya Vovk. "Transduction with confidence and credibility." Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence (IJCAI'99). Vol. 2. 1999.

smaller significance values realte to higher reliability ( epsilon)

confidence is (1-epsilon)

efficiency of a prediction only is important if the prediction holds validity
  -- the confidence of a prediction only matters if we have high credibility for that prediction


